{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb1e5d97-ac7f-4b49-9baa-9ff6d8a13195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b934f2c8-a578-4f09-b5bd-19852f0bd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, shape, weights=None):\n",
    "        self.shape = shape\n",
    "        self.num_layers = len(shape)\n",
    "        if weights is None:\n",
    "            self.weights = []\n",
    "            for i in range(self.num_layers - 1):\n",
    "                W = np.random.uniform(size=(self.shape[i + 1], self.shape[i] + 1))\n",
    "                self.weights.append(W)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "    def run(self, data):\n",
    "        layer = data.T\n",
    "        for i in range(self.num_layers - 1):\n",
    "            prev_layer = layer\n",
    "            o = np.dot(self.weights[i], prev_layer)\n",
    "            # sigmoid\n",
    "            layer = scipy.special.expit(o)\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9739c314-c284-424c-8af2-45e4f6aca4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result(object):\n",
    "    def __init__(self, best_particle, best_scores, accuracies, num_iterations):\n",
    "        self.best_particle = best_particle\n",
    "        self.best_scores = best_scores\n",
    "        self.accuracies = accuracies\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "def dim_weights(shape):\n",
    "    dim = 0\n",
    "    for i in range(len(shape) - 1):\n",
    "        dim = dim + (shape[i] + 1) * shape[i + 1]\n",
    "    return dim\n",
    "\n",
    "def eval_accuracy(weights, shape, X, y):\n",
    "    corrects, wrongs = 0, 0\n",
    "    nn = MultiLayerPerceptron(shape, weights=weights)\n",
    "    predictions = []\n",
    "    for i in range(len(X)):\n",
    "        out_vector = nn.run(X[i])\n",
    "        y_pred = np.argmax(out_vector)\n",
    "        predictions.append(y_pred)\n",
    "        if y_pred == y[i]:\n",
    "            corrects += 1\n",
    "        else:\n",
    "            wrongs += 1\n",
    "    return corrects, wrongs, predictions\n",
    "\n",
    "def weights_to_vector(weights):\n",
    "    w = np.asarray([])\n",
    "    for i in range(len(weights) + 1):\n",
    "        v = weights[i].flatten()\n",
    "        w = np.append(w, v)\n",
    "    return w\n",
    "\n",
    "\n",
    "def vector_to_weights(vector, shape):\n",
    "    weights = []\n",
    "    idx = 0\n",
    "    for i in range(len(shape) - 1):\n",
    "        r = shape[i + 1]\n",
    "        c = shape[i]\n",
    "        idx_min = idx\n",
    "        idx_max = idx + r * c\n",
    "        W = vector[idx_min:idx_max].reshape((r, c))\n",
    "        weights.append(W)\n",
    "        idx = idx_max\n",
    "    return weights\n",
    "\n",
    "def eval_neural_network_via_vector(weights, shape, X, y):\n",
    "    mse = np.asarray([])\n",
    "    weight = vector_to_weights(np.array(weights), shape)\n",
    "    nn = MultiLayerPerceptron(shape, weights=weight)\n",
    "    y_pred = nn.run(X)\n",
    "    mse = np.append(mse, sklearn.metrics.mean_squared_error(np.atleast_2d(y), y_pred))\n",
    "    return mse\n",
    "\n",
    "def eval_neural_network_via_weights(weights, shape, X, y):\n",
    "    mse = np.asarray([])\n",
    "    for w in weights:\n",
    "        weight = vector_to_weights(w, shape)\n",
    "        nn = MultiLayerPerceptron(shape)\n",
    "        nn.weights = weight\n",
    "        y_pred = nn.run(X)\n",
    "        mse = np.append(mse, sklearn.metrics.mean_squared_error(np.atleast_2d(y), y_pred))\n",
    "    return mse\n",
    "\n",
    "\n",
    "def print_best_particle(best_particle):\n",
    "    print(\"New best weights found at iteration #{i} with mean squared error: {score}\".format(i=best_particle[0], score=best_particle[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f9859e6-74a5-4980-818d-b74e450a90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import sklearn.metrics\n",
    "\n",
    "class MLPBatchGradDescent:\n",
    "    def __init__(self, shape, learning_rate=0.1, max_epochs=1000, print_epochs=True):\n",
    "        self.shape = shape\n",
    "        self.num_layers = len(shape)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.print_epochs = print_epochs\n",
    "        self.weights = []\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            W = np.random.uniform(size=(self.shape[i + 1], self.shape[i]))\n",
    "            self.weights.append(W)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(self.num_layers - 2):\n",
    "            activation = scipy.special.expit(np.dot(activations[-1], self.weights[i]))\n",
    "            activations.append(activation)\n",
    "\n",
    "        output = scipy.special.expit(np.dot(activations[-1], self.weights[-1].T))\n",
    "        activations.append(output)\n",
    "        return activations\n",
    "\n",
    "    def backward_propagation(self, X, y, activations):\n",
    "        error = activations[-1] - y\n",
    "        delta = error * activations[-1] * (1 - activations[-1])\n",
    "\n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            self.weights[i] -= self.learning_rate * np.dot(delta.T, activations[i])\n",
    "            hidden_error = np.dot(delta, self.weights[i])\n",
    "            delta = hidden_error * activations[i] * (1 - activations[i])\n",
    "\n",
    "        self.weights[0] -= self.learning_rate * np.dot(delta.T, X)\n",
    "\n",
    "    def get_score(self, X, y):\n",
    "        nn = MLPBatchGradDescent(self.shape, weights=self.weights)\n",
    "        y_pred = nn.run(X)\n",
    "        mse = sklearn.metrics.mean_squared_error(y, y_pred.T)\n",
    "        return mse\n",
    "\n",
    "    def train(self, X, y_onehot, y):\n",
    "        self.initialize_weights()\n",
    "        i = 0\n",
    "        accuracies = []\n",
    "        best_scores = [(i, 1)]\n",
    "        if self.print_epochs:\n",
    "            print_best_particle(i, best_scores[-1])\n",
    "        for epoch in range(self.max_epochs):\n",
    "            activations = self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y_onehot, activations)\n",
    "            i += 1\n",
    "\n",
    "            score = self.get_score(X, y_onehot)\n",
    "            corrects, wrongs, predictions = eval_accuracy(self.weights, self.shape, X, y)\n",
    "            accuracy = corrects / (corrects + wrongs)\n",
    "            best_scores.append((i, score))\n",
    "            if self.print_epochs:\n",
    "                print_best_particle(i, best_scores[-1])\n",
    "                print(\"With accuracy: {accuracy}\".format(accuracy=accuracy))\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        return Result(\n",
    "            best_particle=self.weights,\n",
    "            best_scores=best_scores,\n",
    "            accuracies=accuracies,\n",
    "            num_iterations=self.max_epochs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a243d6fe-69ad-49c2-9d9a-9b9b82ea4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST digits from sklearn\n",
    "num_classes = 10\n",
    "mnist = sklearn.datasets.load_digits(n_class=num_classes)\n",
    "X, X_test, y, y_test = sklearn.model_selection.train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "num_inputs = X.shape[1]\n",
    "\n",
    "y_true = np.zeros((len(y), num_classes))\n",
    "for i in range(len(y)):\n",
    "    y_true[i, y[i]] = 1\n",
    "\n",
    "y_test_true = np.zeros((len(y_test), num_classes))\n",
    "for i in range(len(y_test)):\n",
    "    y_test_true[i, y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0307a3e4-d0a2-4278-af9c-eda9622499e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best weights found at iteration #0 with mean squared error: (0, 1)\n",
      "New best weights found at iteration #1437 with mean squared error: (1437, 0.7979834977856142)\n",
      "With accuracy: 0.11134307585247043\n",
      "New best weights found at iteration #2874 with mean squared error: (2874, 0.7947363386392905)\n",
      "With accuracy: 0.11830201809324982\n",
      "New best weights found at iteration #4311 with mean squared error: (4311, 0.7925942992900239)\n",
      "With accuracy: 0.12108559498956159\n",
      "New best weights found at iteration #5748 with mean squared error: (5748, 0.7912629611950706)\n",
      "With accuracy: 0.12804453723034098\n",
      "New best weights found at iteration #7185 with mean squared error: (7185, 0.790457292146395)\n",
      "With accuracy: 0.13291579679888657\n",
      "New best weights found at iteration #8622 with mean squared error: (8622, 0.7899130211262764)\n",
      "With accuracy: 0.13291579679888657\n",
      "New best weights found at iteration #10059 with mean squared error: (10059, 0.7893522997595921)\n",
      "With accuracy: 0.1336116910229645\n",
      "New best weights found at iteration #11496 with mean squared error: (11496, 0.7884857962649413)\n",
      "With accuracy: 0.12943632567849686\n",
      "New best weights found at iteration #12933 with mean squared error: (12933, 0.7868651522666181)\n",
      "With accuracy: 0.12734864300626306\n",
      "New best weights found at iteration #14370 with mean squared error: (14370, 0.7838094632235615)\n",
      "With accuracy: 0.12526096033402923\n",
      "New best weights found at iteration #15807 with mean squared error: (15807, 0.777421191327736)\n",
      "With accuracy: 0.12317327766179541\n",
      "New best weights found at iteration #17244 with mean squared error: (17244, 0.7351519141183737)\n",
      "With accuracy: 0.11830201809324982\n",
      "New best weights found at iteration #18681 with mean squared error: (18681, 0.6309319807011546)\n",
      "With accuracy: 0.11482254697286012\n",
      "New best weights found at iteration #20118 with mean squared error: (20118, 0.5119355026804144)\n",
      "With accuracy: 0.1127348643006263\n",
      "New best weights found at iteration #21555 with mean squared error: (21555, 0.3866067051065236)\n",
      "With accuracy: 0.10925539318023661\n",
      "New best weights found at iteration #22992 with mean squared error: (22992, 0.3130150503050164)\n",
      "With accuracy: 0.10508002783576896\n",
      "New best weights found at iteration #24429 with mean squared error: (24429, 0.19906748235267246)\n",
      "With accuracy: 0.10160055671537926\n",
      "New best weights found at iteration #25866 with mean squared error: (25866, 0.1937225360996609)\n",
      "With accuracy: 0.10090466249130133\n",
      "New best weights found at iteration #27303 with mean squared error: (27303, 0.19093302406016918)\n",
      "With accuracy: 0.10160055671537926\n",
      "New best weights found at iteration #28740 with mean squared error: (28740, 0.18090664974846607)\n",
      "With accuracy: 0.10786360473208072\n",
      "New best weights found at iteration #30177 with mean squared error: (30177, 0.1282996378515038)\n",
      "With accuracy: 0.1677105080027836\n",
      "New best weights found at iteration #31614 with mean squared error: (31614, 0.12584196488638683)\n",
      "With accuracy: 0.1697981906750174\n",
      "New best weights found at iteration #33051 with mean squared error: (33051, 0.12319114376876698)\n",
      "With accuracy: 0.17884481558803061\n",
      "New best weights found at iteration #34488 with mean squared error: (34488, 0.12043170710386537)\n",
      "With accuracy: 0.19137091162143355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Instantiate and train the MLP\u001b[39;00m\n\u001b[0;32m     17\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPStochasticGradDescent(shape\u001b[38;5;241m=\u001b[39mmlp_shape, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs, print_epochs\u001b[38;5;241m=\u001b[39mprint_epochs)\n\u001b[1;32m---> 18\u001b[0m RESULTS\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient Descent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m])\n",
      "Cell \u001b[1;32mIn[23], line 63\u001b[0m, in \u001b[0;36mMLPStochasticGradDescent.train\u001b[1;34m(self, X, y_onehot, y)\u001b[0m\n\u001b[0;32m     61\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_onehot_shuffled[j:j \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m     62\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(X_batch)\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_score(X, y_onehot)\n",
      "Cell \u001b[1;32mIn[23], line 40\u001b[0m, in \u001b[0;36mMLPStochasticGradDescent.backward_propagation\u001b[1;34m(self, X, y, activations)\u001b[0m\n\u001b[0;32m     37\u001b[0m     hidden_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i])\n\u001b[0;32m     38\u001b[0m     delta \u001b[38;5;241m=\u001b[39m hidden_error \u001b[38;5;241m*\u001b[39m activations[i] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m activations[i])\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RESULTS = []\n",
    "# Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_train_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "# Define the MLP shape and hyperparameters\n",
    "mlp_shape = [X_train.shape[1], 64, num_classes]\n",
    "learning_rate = 0.01\n",
    "max_epochs = 1000\n",
    "print_epochs = True\n",
    "\n",
    "# Instantiate and train the MLP\n",
    "mlp = MLPStochasticGradDescent(shape=mlp_shape, learning_rate=learning_rate, max_epochs=max_epochs, print_epochs=print_epochs)\n",
    "RESULTS.append([\"Gradient Descent\", mlp.train(X_train, y_train_onehot, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3110d-60af-492c-8844-728730324e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
